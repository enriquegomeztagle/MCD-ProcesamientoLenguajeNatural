{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ceff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from contraction_fix import fix\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import datasets as hf_datasets\n",
    "import sklearn\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    from langdetect import detect as _detect_lang\n",
    "except Exception:\n",
    "    _detect_lang = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36c18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\", \"parser\", \"textcat\"])\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@\\w+\")\n",
    "HASHTAG_RE = re.compile(r\"#([\\w√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë]+)\")\n",
    "NON_ALNUM_RE = re.compile(r\"[^0-9A-Za-z√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë‚Äô']+\")\n",
    "CAMEL_RE = re.compile(r\"(?<=[a-z])(?=[A-Z])\")\n",
    "\n",
    "EMOJI_POS = {\n",
    "    \"flexed_biceps\",\n",
    "    \"clapping_hands\",\n",
    "    \"ok_hand\",\n",
    "    \"oncoming_fist\",\n",
    "    \"party_popper\",\n",
    "    \"green_heart\",\n",
    "    \"smiling_face_with_sunglasses\",\n",
    "    \"rocket\",\n",
    "    \"trophy\",\n",
    "    \"raised_fist\",\n",
    "}\n",
    "EMOJI_NEG = {\n",
    "    \"pensive_face\",\n",
    "}\n",
    "EMOJI_NEU = {\n",
    "    \"lion_face\",\n",
    "    \"smirking_face\",\n",
    "    \"smiling_face_with_open_mouth_&_cold_sweat\",\n",
    "    \"winking_face\",\n",
    "    \"hot_pepper\",\n",
    "    \"backhand_index_pointing_right\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffacb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_SW = set(ENGLISH_STOP_WORDS)\n",
    "ES_SW = {\n",
    "    \"de\",\n",
    "    \"la\",\n",
    "    \"que\",\n",
    "    \"el\",\n",
    "    \"en\",\n",
    "    \"y\",\n",
    "    \"a\",\n",
    "    \"los\",\n",
    "    \"del\",\n",
    "    \"se\",\n",
    "    \"las\",\n",
    "    \"por\",\n",
    "    \"un\",\n",
    "    \"para\",\n",
    "    \"con\",\n",
    "    \"no\",\n",
    "    \"una\",\n",
    "    \"su\",\n",
    "    \"al\",\n",
    "    \"lo\",\n",
    "    \"como\",\n",
    "    \"m√°s\",\n",
    "    \"pero\",\n",
    "    \"sus\",\n",
    "    \"le\",\n",
    "    \"ya\",\n",
    "    \"o\",\n",
    "    \"este\",\n",
    "    \"s√≠\",\n",
    "    \"porque\",\n",
    "    \"esta\",\n",
    "}\n",
    "IT_SW = {\n",
    "    \"di\",\n",
    "    \"a\",\n",
    "    \"da\",\n",
    "    \"in\",\n",
    "    \"che\",\n",
    "    \"la\",\n",
    "    \"e\",\n",
    "    \"il\",\n",
    "    \"le\",\n",
    "    \"i\",\n",
    "    \"un\",\n",
    "    \"una\",\n",
    "    \"per\",\n",
    "    \"con\",\n",
    "    \"non\",\n",
    "    \"su\",\n",
    "    \"al\",\n",
    "    \"lo\",\n",
    "    \"gli\",\n",
    "    \"del\",\n",
    "    \"della\",\n",
    "    \"dei\",\n",
    "    \"delle\",\n",
    "}\n",
    "FR_SW = {\n",
    "    \"de\",\n",
    "    \"la\",\n",
    "    \"le\",\n",
    "    \"les\",\n",
    "    \"des\",\n",
    "    \"et\",\n",
    "    \"√†\",\n",
    "    \"a\",\n",
    "    \"en\",\n",
    "    \"un\",\n",
    "    \"une\",\n",
    "    \"pour\",\n",
    "    \"avec\",\n",
    "    \"pas\",\n",
    "    \"sur\",\n",
    "    \"au\",\n",
    "    \"aux\",\n",
    "    \"du\",\n",
    "    \"dans\",\n",
    "    \"ce\",\n",
    "    \"cet\",\n",
    "    \"cette\",\n",
    "    \"ces\",\n",
    "}\n",
    "STOPWORDS_ALL = EN_SW | ES_SW | IT_SW | FR_SW | {\"amp\", \"rt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b5dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_hashtag_token(tok: str) -> str:\n",
    "    t = CAMEL_RE.sub(\" \", tok)\n",
    "    t = re.sub(r\"gp$\", \" gp\", t, flags=re.IGNORECASE)\n",
    "    return t.lower()\n",
    "\n",
    "\n",
    "def _detect_language(text: str) -> str:\n",
    "    if _detect_lang is None:\n",
    "        return \"en\"\n",
    "    try:\n",
    "        return _detect_lang(text)\n",
    "    except Exception:\n",
    "        return \"en\"\n",
    "\n",
    "\n",
    "def clean_and_expand(text: str) -> str:\n",
    "    t = (text or \"\").lower()\n",
    "    t = fix(t)\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "    t = HASHTAG_RE.sub(lambda m: \" \" + _split_hashtag_token(m.group(1)) + \" \", t)\n",
    "    t = NON_ALNUM_RE.sub(\" \", t)\n",
    "    raw_tokens = t.split()\n",
    "    mapped = []\n",
    "    for w in raw_tokens:\n",
    "        if w in EMOJI_POS:\n",
    "            mapped.append(\"emopos\")\n",
    "        elif w in EMOJI_NEG:\n",
    "            mapped.append(\"emoneg\")\n",
    "        else:\n",
    "            mapped.append(w)\n",
    "    tokens = [w for w in mapped if len(w) > 1 and w not in STOPWORDS_ALL]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for tok in doc:\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if (\n",
    "            len(lemma) > 1\n",
    "            and any(c.isalpha() for c in lemma)\n",
    "            and lemma not in STOPWORDS_ALL\n",
    "        ):\n",
    "            out.append(lemma)\n",
    "    return \" \".join(out)\n",
    "\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    cleaned = clean_and_expand(text)\n",
    "    lang = _detect_language(text)\n",
    "    if lang == \"en\":\n",
    "        return lemmatize_text(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52dcb439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes!!! Exactly the start we hoped for flexed_b...</td>\n",
       "      <td>1</td>\n",
       "      <td>yes exactly start hope flexed bicep lovely res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pole Position!! Very happy that we managed to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>pole position happy manage good bit qualify to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51 years old today and my biggest support ever...</td>\n",
       "      <td>1</td>\n",
       "      <td>year old today big support clapping hand happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We have a lot of work to do. Not the first rac...</td>\n",
       "      <td>0</td>\n",
       "      <td>lot work race want struggle pace race unfortun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Starting third tomorrow. Not a bad day all in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>start tomorrow bad day let ‚Äôs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Excelente inicio de Campeonato. Gran trabajo d...</td>\n",
       "      <td>0</td>\n",
       "      <td>excelente inicio campeonato gran trabajo honda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>¬°Muy buen inicio bloqueando la primera fila pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>muy buen inicio bloqueando primera fila equipo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We‚Äôre a long way from where we want to be righ...</td>\n",
       "      <td>0</td>\n",
       "      <td>‚Äôre long way want right lack commitment single...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P6 in quali is not where we want to be, but we...</td>\n",
       "      <td>0</td>\n",
       "      <td>p6 quali want know build tomorrow count ‚Äôre mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>üáßüá≠ Domingo dif√≠cil, pero ya sab√≠amos que Bar√©i...</td>\n",
       "      <td>0</td>\n",
       "      <td>domingo dif√≠cil sab√≠amos bar√©in iba ser duro t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Yes!!! Exactly the start we hoped for flexed_b...      1   \n",
       "1  Pole Position!! Very happy that we managed to ...      1   \n",
       "2  51 years old today and my biggest support ever...      1   \n",
       "3  We have a lot of work to do. Not the first rac...      0   \n",
       "4  Starting third tomorrow. Not a bad day all in ...      0   \n",
       "5  Excelente inicio de Campeonato. Gran trabajo d...      0   \n",
       "6  ¬°Muy buen inicio bloqueando la primera fila pa...      0   \n",
       "7  We‚Äôre a long way from where we want to be righ...      0   \n",
       "8  P6 in quali is not where we want to be, but we...      0   \n",
       "9  üáßüá≠ Domingo dif√≠cil, pero ya sab√≠amos que Bar√©i...      0   \n",
       "\n",
       "                                               clean  \n",
       "0  yes exactly start hope flexed bicep lovely res...  \n",
       "1  pole position happy manage good bit qualify to...  \n",
       "2  year old today big support clapping hand happy...  \n",
       "3  lot work race want struggle pace race unfortun...  \n",
       "4                      start tomorrow bad day let ‚Äôs  \n",
       "5  excelente inicio campeonato gran trabajo honda...  \n",
       "6  muy buen inicio bloqueando primera fila equipo...  \n",
       "7  ‚Äôre long way want right lack commitment single...  \n",
       "8  p6 quali want know build tomorrow count ‚Äôre mi...  \n",
       "9  domingo dif√≠cil sab√≠amos bar√©in iba ser duro t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"Malekith/twitter_f1\")\n",
    "df = pd.concat([ds[s].to_pandas() for s in ds], ignore_index=True)[[\"text\", \"label\"]]\n",
    "df[\"clean\"] = df[\"text\"].astype(str).apply(normalize)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92830ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "TF-IDF MATRIX INFO\n",
      "--------------------------------------------------\n",
      "\n",
      "TF-IDF matrix shape: (4538, 2533)\n",
      "Vocabulary size: 2533\n",
      "\n",
      "--------------------------------------------------\n",
      "SPARSE SAMPLE VECTORS\n",
      "--------------------------------------------------\n",
      "Saved outputs/tfidf_sample_sparse.csv (non-zero TF-IDF entries for sample docs)\n",
      "         feature     tfidf  doc_id\n",
      "0    entire team  0.277262       0\n",
      "1         lovely  0.265017       0\n",
      "2      team hard  0.265017       0\n",
      "3  clapping hand  0.260075       0\n",
      "4         entire  0.257818       0\n",
      "5       let push  0.240374       0\n",
      "6      big thank  0.227291       0\n",
      "7            yes  0.221642       0\n",
      "8      hard work  0.209631       0\n",
      "9       clapping  0.207123       0\n",
      "\n",
      "--------------------------------------------------\n",
      "DENSE SAMPLE VECTORS\n",
      "--------------------------------------------------\n",
      "Saved outputs/tfidf_sample_dense_first5.csv (dense TF-IDF matrix for first 5 docs)\n",
      "   100th  10th  11th  1sec  1st  2nd  2nd row  2sec  3rd  3sec  ...  \\\n",
      "0    0.0   0.0   0.0   0.0  0.0  0.0      0.0   0.0  0.0   0.0  ...   \n",
      "1    0.0   0.0   0.0   0.0  0.0  0.0      0.0   0.0  0.0   0.0  ...   \n",
      "2    0.0   0.0   0.0   0.0  0.0  0.0      0.0   0.0  0.0   0.0  ...   \n",
      "\n",
      "   yesterday  yesterday today   yo  zandvoort  zero  zhou   zu  √∫ltima  \\\n",
      "0        0.0              0.0  0.0        0.0   0.0   0.0  0.0     0.0   \n",
      "1        0.0              0.0  0.0        0.0   0.0   0.0  0.0     0.0   \n",
      "2        0.0              0.0  0.0        0.0   0.0   0.0  0.0     0.0   \n",
      "\n",
      "   √∫ltima vuelta  √∫ltimo  \n",
      "0            0.0     0.0  \n",
      "1            0.0     0.0  \n",
      "2            0.0     0.0  \n",
      "\n",
      "[3 rows x 2533 columns]\n",
      "Saved sample/vector artifacts to outputs/: tfidf_sample_sparse.csv, tfidf_sample_dense_first5.csv, vocabulary.csv, clean_texts.csv\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    lowercase=False,\n",
    "    strip_accents=None,\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"clean\"])\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TF-IDF MATRIX INFO\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nTF-IDF matrix shape:\", X.shape)\n",
    "print(\"Vocabulary size:\", len(features))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def sparse_view(i):\n",
    "    row = X.getrow(i)\n",
    "    nz = row.nonzero()[1]\n",
    "    return pd.DataFrame({\"feature\": features[nz], \"tfidf\": row.data}).sort_values(\n",
    "        \"tfidf\", ascending=False\n",
    "    )\n",
    "\n",
    "\n",
    "sample_count = min(3, X.shape[0])\n",
    "sample_sparse = pd.concat(\n",
    "    [sparse_view(i).assign(doc_id=i) for i in range(sample_count)], ignore_index=True\n",
    ")\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"SPARSE SAMPLE VECTORS\")\n",
    "print(\"-\" * 50)\n",
    "sample_sparse.to_csv(output_dir / \"tfidf_sample_sparse.csv\", index=False)\n",
    "print(\"Saved outputs/tfidf_sample_sparse.csv (non-zero TF-IDF entries for sample docs)\")\n",
    "print(sample_sparse.head(10))\n",
    "\n",
    "dense_rows = min(5, X.shape[0])\n",
    "dense_slice = pd.DataFrame(X[:dense_rows].toarray(), columns=features)\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"DENSE SAMPLE VECTORS\")\n",
    "print(\"-\" * 50)\n",
    "dense_slice.to_csv(output_dir / \"tfidf_sample_dense_first5.csv\", index=False)\n",
    "print(\n",
    "    \"Saved outputs/tfidf_sample_dense_first5.csv (dense TF-IDF matrix for first 5 docs)\"\n",
    ")\n",
    "print(dense_slice.head(3))\n",
    "\n",
    "pd.DataFrame({\"term\": features}).to_csv(output_dir / \"vocabulary.csv\", index=False)\n",
    "df[[\"label\", \"clean\"]].to_csv(output_dir / \"clean_texts.csv\", index=False)\n",
    "\n",
    "print(\n",
    "    \"Saved sample/vector artifacts to outputs/:\",\n",
    "    \"tfidf_sample_sparse.csv,\",\n",
    "    \"tfidf_sample_dense_first5.csv,\",\n",
    "    \"vocabulary.csv,\",\n",
    "    \"clean_texts.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a2110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "TOP TERMS PER DOCUMENT\n",
      "--------------------------------------------------\n",
      "\n",
      "Doc 0 top terms: [('entire team', 0.2772619348271273), ('team hard', 0.26501653242254547), ('lovely', 0.26501653242254547), ('clapping hand', 0.26007489547975754), ('entire', 0.25781819176685666), ('let push', 0.24037394260799597), ('big thank', 0.22729144383365152), ('yes', 0.22164159836964992), ('hard work', 0.20963085678321863), ('clapping', 0.20712324803616872)]\n",
      "\n",
      "Doc 1 top terms: [('hand', 0.274558060382315), ('team clap', 0.26316977279200177), ('clap hand', 0.24725593278476443), ('great work', 0.24725593278476443), ('today great', 0.23633575942682591), ('forward race', 0.23406343751680025), ('work team', 0.23192891918554107), ('ok hand', 0.21835021782119016), ('pole position', 0.21564051426880734), ('clap', 0.20650344442271162)]\n",
      "\n",
      "Doc 2 top terms: [('party', 0.36628431918674614), ('popper', 0.36628431918674614), ('old', 0.35247155986217465), ('happy birthday', 0.33300356901700945), ('clapping hand', 0.3162799807236501), ('birthday', 0.3017662750345259), ('clapping', 0.2518848917557212), ('support', 0.22931815008455014), ('big', 0.22795273181944045), ('year', 0.21823621661913728)]\n",
      "\n",
      "--------------------------------------------------\n",
      "CORPUS STATS\n",
      "--------------------------------------------------\n",
      "\n",
      "Documents: 4538\n",
      "Unique terms (vocab size): 2533\n"
     ]
    }
   ],
   "source": [
    "def top_terms(row_idx, k=10):\n",
    "    row = X.getrow(row_idx)\n",
    "    nz = row.nonzero()[1]\n",
    "    vals = row.data\n",
    "    order = np.argsort(-vals)[:k]\n",
    "    return [(features[nz[i]], float(vals[i])) for i in order]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TOP TERMS PER DOCUMENT\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(3):\n",
    "    print(f\"\\nDoc {i} top terms:\", top_terms(i, 10))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"CORPUS STATS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nDocuments:\", len(df))\n",
    "print(\"Unique terms (vocab size):\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a95fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
